## 为什么需要深度学习编译器

从模型框架选择到项目落地，需要部署的设备是五花八门的，需要在所有可能的部署设备上都达到**良好的性能并且易于使用**是一件非常困难的事。

一般会使用硬件厂商自己推出的一些前向推理框架，在Intel的CPU/GPU上就使用OpenVINO，在Arm的CPU/GPU上使用NCNN/MNN等，在Nvidia GPU上使用TensorRT。

问题：需要将训练的模型分别转换到特定框架可以读取的格式且要考虑OP实现是否完全对齐的问题。

- 编译器前端：接收C/C++/Java等不同语言，进行代码生成，吐出IR
- 编译器中端：接收IR，进行不同编译器后端可以共享的优化，如常量替换，死代码消除，循环优化等，吐出优化后的IR
- 编译器后端：接收优化后的IR，进行不同硬件的平台相关优化与硬件指令生成，吐出目标文件

受到编译器解决方法的启发，深度学习编译器被提出，我们可以将**各个训练框架训练出来的模型看作各种编程语言**，然后将这些模型传入深度学习编译器之后吐出IR，由于深度学习的IR其实就是计算图，所以可以直接叫作**Graph IR**。针对这些Graph IR可以做一些计算图优化再吐出IR分发给各种硬件使用。这样，深度学习编译器的过程就和传统的编译器类似，可以解决上面提到的很多繁琐的问题。

## TVM

TVM就是一个基于编译优化的深度学习推理框架。



![图片](https://wangyidipicgo.oss-cn-hangzhou.aliyuncs.com/img/202212261959636.png)

## TVM编译推理测试

见Jupyter文件

